{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e7d9baf-a5f6-4aee-a150-734bcaf415e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "275c57c4-f47c-4d05-8899-756971622bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_filename = \"multi_config_v1.json\"\n",
    "\n",
    "config = dict()\n",
    "\n",
    "model_config = dict()\n",
    "model_config[\"name\"] = \"DynUNet\"  # network model name from MONAI\n",
    "# set the network hyper-parameters\n",
    "model_config[\"in_channels\"] = 4  # 4 input images for the BraTS challenge\n",
    "model_config[\"out_channels\"] = 1   # whole tumor, tumor core, enhancing tumor\n",
    "model_config[\"spatial_dims\"] = 3   # 3D input images\n",
    "model_config[\"deep_supervision\"] = False  # do not check outputs of lower layers\n",
    "model_config[\"strides\"] = [[1, 1, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]][:-1]  # number of downsampling convolutions\n",
    "model_config[\"filters\"] = [64, 96, 128, 192, 256, 384, 512, 768, 1024][:len(model_config[\"strides\"])]  # number of filters per layer\n",
    "model_config[\"kernel_size\"] = [[3, 3, 3]] * len(model_config[\"strides\"])  # size of the convolution kernels per layer\n",
    "model_config[\"upsample_kernel_size\"] = model_config[\"strides\"][1:]  # should be the same as the strides\n",
    "\n",
    "# put the model config in the main config\n",
    "config[\"model\"] = model_config\n",
    "\n",
    "config[\"optimizer\"] = {'name': 'Adam', \n",
    "                       'lr': 0.001}  # initial learning rate\n",
    "\n",
    "# define the loss\n",
    "config[\"loss\"] = {'name': 'DiceLoss', # from Monai\n",
    "                  'include_background': True,  # we do not have a label for the background, so this should be true (by \"include background\" monai means include channel 0)\n",
    "                  'sigmoid': True,\n",
    "                  'batch': True}  # transform the model logits to activations\n",
    "\n",
    "# set the cross validation parameters\n",
    "config[\"cross_validation\"] = {'folds': 5,  # number of cross validation folds\n",
    "                              'seed': 25},  # seed to make the generation of cross validation folds consistent across different trials\n",
    "# set the scheduler parameters\n",
    "config[\"scheduler\"] = {'name': 'ReduceLROnPlateau', \n",
    "                       'patience': 5,  # wait 5 epochs with no improvement before reducing the learning rate\n",
    "                       'factor': 0.5,   # multiply the learning rate by 0.5\n",
    "                       'min_lr': 1e-08}  # stop reducing the learning rate once it gets to 1e-8\n",
    "\n",
    "# set the dataset parameters\n",
    "roi_size = [192, 192, 96]\n",
    "config[\"dataset\"] = {'name': 'SegmentationDatasetPersistent',  # 'Persistent' means that it will save the preprocessed outputs generated during the first epoch\n",
    "# However, using 'Persistent', does also increase the time of the first epoch compared to the other epochs, which should run faster\n",
    "  'labels': [1],  # 1: tumor \n",
    "  'normalization': 'ScaleIntensityD',  # scale intensity from 0 to 1\n",
    "  'normalization_kwargs': {'channel_wise': True},  # perform the normalization channel wise\n",
    "  'orientation': 'RAS',  # Force all the images to be the same orientation (Right-Anterior-Suppine)\n",
    "  'training':  # the following arguments will only be applied to the training data.\n",
    "    {\n",
    "    'desired_shape': roi_size,  # resize the images to this shape, increase this to get higher resolution images (increases computation time and memory usage)\n",
    "    'random_crop': True,  # resample the images when resizing them, otherwise the resize could crop out regions of interest\n",
    "    'crop_foreground': True,  # crop the foreground of the images\n",
    "    'foreground_percentile': 0.75,  # aggressive foreground cropping to make sure the empty space is taken out of the images\n",
    "    'spatial_augmentations': [{'name': 'RandFlipD', 'spatial_axis': 0, 'prob': 0.5},\n",
    "                              {'name': 'RandFlipD', 'spatial_axis': 1, 'prob': 0.5}],\n",
    "    'intensity_augmentations': [{'name': 'RandScaleIntensityD', 'factors': 0.1, 'prob': 1.0},\n",
    "                                {'name': 'RandShiftIntensityD', 'offsets': 0.1, 'prob': 1.0}],\n",
    "    }\n",
    "}\n",
    "\n",
    "config[\"inference\"] = {'name': 'SlidingWindowInferer',\n",
    "                       'roi_size': roi_size,\n",
    "                       'mode': 'gaussian'}  # sliding window inference for validation\n",
    "\n",
    "config[\"training\"] = {'batch_size': 4,  # number of image/label pairs to read at a time during training\n",
    "  'validation_batch_size': 1,  # number of image/label pairs to read at atime during validation\n",
    "  'amp': False,  # don't set this to true unless the model you are using is setup to use automatic mixed precision (AMP)\n",
    "  'early_stopping_patience': None,  # stop the model early if the validaiton loss stops improving\n",
    "  'n_epochs': 100,  # number of training epochs, reduce this if you don't want training to run as long\n",
    "  'save_every_n_epochs': None,  # save the model every n epochs (otherwise only the latest model will be saved)\n",
    "  'save_last_n_models': None,  # save the last n models \n",
    "  'save_best': True, # save the model that has the best validation loss\n",
    "  'training_iterations_per_epoch': 10}  # validation takes a long time, so I don't want to validate every training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59f8992a-ef74-4cf2-8635-3506ebb5c840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aligned/PT_77/20210507/PT_77_T2)29210507.nii.gz\n",
      "aligned/PT_80/20190410/PT_80_DWI_b0.nii.gz\n",
      "aligned/PT_80/20190410/PT_80_DWI_b100.nii.gz\n",
      "aligned/PT_81/20200129/PT_81_T2_20200129.nii.gz\n"
     ]
    }
   ],
   "source": [
    "# get the training filenames\n",
    "config[\"training_filenames\"] = list()\n",
    "ground_truth_filenames = sorted(glob.glob(\"./aligned/*/*/*NB*.nii*\"))\n",
    "for label_filename in ground_truth_filenames:\n",
    "    subject, visit = label_filename.split(\"/\")[-3:-1]\n",
    "    filenames = sorted(glob.glob(os.path.join(\"aligned\", subject, visit, f\"*.nii*\")))\n",
    "    n_features = len(filenames) \n",
    "    feature_modalities = [\"_\".join(fn.split(\"_\")[3:-1]).strip(\"8 \") for fn in filenames]\n",
    "    t1_filename = filenames[feature_modalities.index(\"T1_gd\")]\n",
    "    assert os.path.exists(t1_filename)\n",
    "    \n",
    "    if len(feature_modalities) < 5:\n",
    "        continue\n",
    "    \n",
    "    if \"T2\" not in feature_modalities:\n",
    "        for filename in filenames:\n",
    "            if \"T2\" in filename:\n",
    "                t2_fn = filename\n",
    "        print(t2_fn)\n",
    "    else:\n",
    "        t2_fn = filenames[feature_modalities.index(\"T2\")]\n",
    "    assert os.path.exists(t2_fn)\n",
    "    \n",
    "    if \"DWI_b0\" not in feature_modalities:\n",
    "        for filename in filenames:\n",
    "            if \"DWI\" in filename and \"b0\" in filename:\n",
    "                dwi_b0 = filename\n",
    "        print(dwi_b0)\n",
    "    else:\n",
    "        dwi_b0 = filenames[feature_modalities.index(\"DWI_b0\")]\n",
    "    assert os.path.exists(dwi_b0)\n",
    "\n",
    "    if \"DWI_b100\" not in feature_modalities:\n",
    "        for filename in filenames:\n",
    "            if \"DWI\" in filename and \"b100\" in filename:\n",
    "                dwi_b100 = filename\n",
    "        print(dwi_b100)\n",
    "    else:\n",
    "        dwi_b100 = filenames[feature_modalities.index(\"DWI_b100\")]\n",
    "    assert os.path.exists(dwi_b100)\n",
    "\n",
    "    \n",
    "    config[\"training_filenames\"].append({\"image\": [t1_filename, t2_fn, dwi_b0, dwi_b100], \"label\": label_filename})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07e76082-324d-48e7-a8c9-44e49631bad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(config[\"training_filenames\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a0e0b8d7-b3f6-4b28-a919-5cc278891ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(config_filename, \"w\") as op:\n",
    "    json.dump(config, op, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d635538f-6333-401f-91a1-568c149dd4d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efece25-d4d6-4481-87b8-9cf63e161251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48158b9-2d21-429a-be3f-5f0aa4cb7a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "93ed3a60-3492-4ad7-83e5-b0d56dfeccfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aligned/PT_80/20190410/PT_80_DWI_b0.nii.gz',\n",
       " 'aligned/PT_80/20190410/PT_80_DWI_b100.nii.gz',\n",
       " 'aligned/PT_80/20190410/PT_80_NB_20190410.nii.gz',\n",
       " 'aligned/PT_80/20190410/PT_80_T1_gd_20190410.nii',\n",
       " 'aligned/PT_80/20190410/PT_80_T2_20190410.nii.gz']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e9f49c73-44c9-4a5c-994c-fc5759c57b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DWI', 'DWI', 'NB', 'T1_gd', 'T2']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e5f01a52-8b65-4c97-bbd1-c4b29011eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cb18f59-5ba4-400f-838f-b209b8ccda8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.asarray([1, np.nan, 2, np.nan, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a7be936-8284-4888-81f1-fc6a5f7e8a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4250fed5-5b82-49bf-8576-54da8cbb5af2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "quantile() q must be in the range [0, 1] but got 1.1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: quantile() q must be in the range [0, 1] but got 1.1"
     ]
    }
   ],
   "source": [
    "torch.quantile(torch.rand(100), 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dbab2ece-bb05-4ac3-be15-15ae9690c06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "image = nib.load(\"./T1w_config_v2/fold2/validation/PT_16_T1_gd_20220712.nii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f521abdc-6aa7-4287-a740-b5e1be912173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 384, 280)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f89bd3c-36c3-451d-bf5f-1a88ee5b5b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.asarray(image.dataobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dab0a8a8-1886-4560-b522-aa9957546f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_image = nib.load(\"./train/PT_16/20220712/PT_16_NB_20220712.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c3dacd68-f1ba-4513-bd76-67e705289def",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_data = np.asarray(gt_image.dataobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8466b1bd-2a4a-45e3-9451-621cf6a12975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int16)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(gt_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ac0e8235-d686-41b2-b891-22b942ef6198",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_data = np.asarray(gt_data > 0, np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c835519-274d-49df-9595-5b5254e4c751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.losses.dice import DiceLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "67548dfd-8c0e-4078-955c-bf1003feaf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = DiceLoss(sigmoid=True, batch=False, include_background=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f7f3505c-635c-4e36-889f-50f201c0a257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9633)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crit(torch.from_numpy(data)[None, None], torch.from_numpy(gt_data)[None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1871eea9-a7e0-47ca-ac09-4cb7089d4058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.63820795679383"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eff0aaad-45fa-47c1-9726-1ad9f268e00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "baaa4312-ecd6-4be9-8a1a-4643bf597bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(torch.flatten(torch.from_numpy(np.asarray(data, float)), start_dim=-3), 75, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92fdfed9-29c6-47e2-9ba0-dc4dc45400d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "kthvalue(input, k, dim=None, keepdim=False, *, out=None) -> (Tensor, LongTensor)\n",
       "\n",
       "Returns a namedtuple ``(values, indices)`` where ``values`` is the :attr:`k` th\n",
       "smallest element of each row of the :attr:`input` tensor in the given dimension\n",
       ":attr:`dim`. And ``indices`` is the index location of each element found.\n",
       "\n",
       "If :attr:`dim` is not given, the last dimension of the `input` is chosen.\n",
       "\n",
       "If :attr:`keepdim` is ``True``, both the :attr:`values` and :attr:`indices` tensors\n",
       "are the same size as :attr:`input`, except in the dimension :attr:`dim` where\n",
       "they are of size 1. Otherwise, :attr:`dim` is squeezed\n",
       "(see :func:`torch.squeeze`), resulting in both the :attr:`values` and\n",
       ":attr:`indices` tensors having 1 fewer dimension than the :attr:`input` tensor.\n",
       "\n",
       ".. note::\n",
       "    When :attr:`input` is a CUDA tensor and there are multiple valid\n",
       "    :attr:`k` th values, this function may nondeterministically return\n",
       "    :attr:`indices` for any of them.\n",
       "\n",
       "Args:\n",
       "    input (Tensor): the input tensor.\n",
       "    k (int): k for the k-th smallest element\n",
       "    dim (int, optional): the dimension to find the kth value along\n",
       "    keepdim (bool): whether the output tensor has :attr:`dim` retained or not.\n",
       "\n",
       "Keyword args:\n",
       "    out (tuple, optional): the output tuple of (Tensor, LongTensor)\n",
       "                           can be optionally given to be used as output buffers\n",
       "\n",
       "Example::\n",
       "\n",
       "    >>> x = torch.arange(1., 6.)\n",
       "    >>> x\n",
       "    tensor([ 1.,  2.,  3.,  4.,  5.])\n",
       "    >>> torch.kthvalue(x, 4)\n",
       "    torch.return_types.kthvalue(values=tensor(4.), indices=tensor(3))\n",
       "\n",
       "    >>> x=torch.arange(1.,7.).resize_(2,3)\n",
       "    >>> x\n",
       "    tensor([[ 1.,  2.,  3.],\n",
       "            [ 4.,  5.,  6.]])\n",
       "    >>> torch.kthvalue(x, 2, 0, True)\n",
       "    torch.return_types.kthvalue(values=tensor([[4., 5., 6.]]), indices=tensor([[1, 1, 1]]))\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.kthvalue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74879b45-117d-422f-9e33-935bc6f0b247",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (brats)",
   "language": "python",
   "name": "brats"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
